20150807211841
/cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS /cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS 

Synopsis:	mpirun [options] {-srun | <cmd> | -f <app> }
Description:	Startup an MPI application.
Run styles:

Options (all platforms):
		-help               Print this message.
		-d                  Turn on debugging.
		-version            Print version number.
		-j                  Print the job ID.
		-p                  Pretend to take action.
		-ck                 Pretend to take action, but check host
		                    connectivity and program availability.
		-dbgspin            Application spins in MPI_Init() until user
		                    sets mpi_debug_cont.  See mpidebug(1).
		-v                  Be verbose.
		-i <spec>           Set instrumentation specification.
		-spawn              Include runtime support for spawn.
		-1sided             Include runtime support for 1sided.
		-commd              Route off-host messages via comm daemon.
		-subnet <net-path>  Use subnet associated with hostname
		                    or ip address as returned by
		                    'hostname -i <net-path>'
		-netaddr <ip-designations>
		                    Specifies what IP addresses to use,
		                    like -subnet but with more control
		                    ip-designation is comma separated:
		                    rank:IP[/mask-IP]   - for rank-rank
		                    mpirun:IP[/mask-IP] - for mpirun-*
		                    IP[/mask-IP]        - for both
		-hostlist <quoted-list-of-hosts>
		                    Constructs an appfile using the
		                    specified hosts.  Can be used with
		                    the -np # option.
		-hostfile <filename-of-hosts>
		-machinefile <filename-of-hosts>
		                    Constructs an appfile using the
		                    specified hosts from the file.  Can
		                    be used with the -np # option.
		                    'machinefile' is a synonym for 'hostfile'.
		-lsb_hosts | -lsb_mcpu_hosts
		                    Constructs an appfile using the
		                    LSB_HOSTS or LSB_MCPU_HOSTS env
		                    vars.  Can be used with -np #.
		                    Implies '-lsf' flag
		-lsf
		                    Use 'blaunch' instead of ssh/
		                    Windows Remote Launch service to
		                    launch remote processes.
		-dd|-ndd            Turn on/off deferred memory deregistration
		-TCP                Force to use tcp/ip interconnect
		-srq|-rdma|-xrc|-sndrcv 
		                    -srq: Use short message shared receive
		                    queue(SRQ) protocol if available.
		                    -rdma: Use short message RDMA protocol.
		                    -sndrcv: Use short message Send-Receive 
		                          protocol.
		                    -xrc: Extended Reliable Connection AND -srq
		                    (-xrc not supported by IBM Platform MPI for Windows.
		-intra=shm|nic|mix
		                    Specifies what communication method to
		                    use for intra-host messages.  The default
		                    is shared memory (shm) which should have
		                    the best latency.  The second setting (nic)
		                    causes the same interconnect being used for
		                    inter-host transfers to be used for intra-
		                    host transfers.  On some systems and some
		                    interconnects this might give higher
		                    bandwidth.  The third setting (mix) causes
		                    shared memory to be used for short messages
		                    (below 256K, or MPI_RDMA_INTRALEN if
		                    specified) and the interconnect for longer
		                    messages.
		                    will be the best setting, nic is only
		                    provided for completeness; mix is however
		                    not supported on elan/psm/mx/tcp
		-T                  Print ranks elapsed usr/sys times(2) data.
		-prot               Print ranks communication protocols.
		-cpu_bind[_mt][=arg]   Binds CPUs to ranks for ccNUMA arch.
		-np #               Start '#' processes using 'file'.
		-e <var>[=<val>]    Set env. var. for the processes.
		-sp <paths>         Use <paths> to locate executables.
		-mpi32 | mpi64      Used to indicate application bitness.
		                    This is required for some IBM Platform MPI 
		                    functionality.
		-stdio=<modes>      Set StandardIO modes (See MPI StandardIO
		                    manpage).
		-universe_size=<#>  Set MPI_UNIVERSE_SIZE attribute.

		-f <app>            Use <app> as application file.
		-- [<extra_args>]   Use the remainder of the command line as
		                    extra arguments to each <cmd> in <app>.

		<cmd>               Use <cmd> as SPMD command line.

Options (Linux Only):
		-tv                 Run the application using the TotalView
		                    debugger.
		-ha[:opts]          Turn on High/Availability mode. 
		-prun <options>     Use prun for quadrics support.
		-srun <options>     Use srun for XC support.
		-itapi|-ITAPI       (HP-UX) Causes the ITAPI (IB) protocol
		                    to be used if available.  Upper case option
		                    causes exit if protocol not available
		-hmp|-HMP           (HP-UX) No longer supported.
		-elan|-ELAN         Causes the ELAN protocol (Quadrics)
		                    to be used if available.  Upper case option
		                    causes exit if protocol not available
		-ibv|-IBV           Causes the IBV protocol (OpenFabric)
		                    to be used if available.  Upper case option
		                    causes exit if protocol not available
		-vapi|-VAPI         Causes the VAPI protocol (Mellanox IB)
		                    to be used if available.  Upper case option
		                    causes exit if protocol not available
		-udapl|-UDAPL       Causes the uDAPL protocol (IB + others)
		                    to be used if available.  Upper case option
		                    causes exit if protocol not available
		-gm|-GM             Causes the GM protocol (Myrinet) to be
		                    used if available.  Upper case option
		                    causes exit if protocol not available
		-mx|-MX             Causes the MX protocol (Myrinet) to be
		                    used if available.  Upper case option
		                    causes exit if protocol not available
		-psm|-PSM           Causes the PSM protocol (QLogic) to be
		                    used if available.  Upper case option
		                    causes exit if protocol not available

Application File:	            One per line: [options] <cmd>
		                    Comment lines start with '#'.

Options:	-np,
		-e, -sp             See above.
		<cmd>               Binary program name and arguments.
NOTE: Not all options are supported on all architectures.
IOR-3.0.1: MPI Coordinated Test of Parallel I/O

Began: Fri Aug  7 21:18:44 2015
Command line used: /cosma5/data/dr002/dc-kash1/ior/bin/ior -vvv -b 16g -f /cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS/con.ior
Machine: Linux m5140
Start time skew across all tasks: 0.00 sec

Test 0 started: Fri Aug  7 21:18:44 2015
Path: /cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS
FS: 2435.9 TiB   Used FS: 81.7%   Inodes: 1280.0 Mi   Used Inodes: 5.9%
Participating tasks: 16
Summary:
	api                = MPIIO (version=2, subversion=2)
	test filename      = /cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS/iorTest
	access             = file-per-process, independent
	pattern            = segmented (1 segment)
	ordering in a file = sequential offsets
	ordering inter file= no tasks offsets
	clients            = 16 (1 per node)
	repetitions        = 1
	xfersize           = 4 MiB
	blocksize          = 16 GiB
	aggregate filesize = 256 GiB

access    bw(MiB/s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter
------    ---------  ---------- ---------  --------   --------   --------   --------   ----
delaying 10 seconds . . .

hints passed to MPI_File_open() {
}

hints returned from opened file {
	cb_buffer_size = 4194304
	romio_cb_read = automatic
	romio_cb_write = automatic
	cb_nodes = 1
	romio_no_indep_rw = false
	ind_rd_buffer_size = 4194304
	ind_wr_buffer_size = 524288
	romio_ds_read = automatic
	romio_ds_write = automatic
	cb_config_list = *:1
}
write     2612.58    16777216   4096       0.023423   100.32     23.56      100.34     0   
delaying 10 seconds . . .

hints passed to MPI_File_open() {
}

hints returned from opened file {
	cb_buffer_size = 4194304
	romio_cb_read = automatic
	romio_cb_write = automatic
	cb_nodes = 1
	romio_no_indep_rw = false
	ind_rd_buffer_size = 4194304
	ind_wr_buffer_size = 524288
	romio_ds_read = automatic
	romio_ds_write = automatic
	cb_config_list = *:1
}
read      1383.85    16777216   4096       0.000513   189.43     15.90      189.43     0   
remove    -          -          -          -          -          -          0.029898   0   

Max Write: 2612.58 MiB/sec (2739.49 MB/sec)
Max Read:  1383.85 MiB/sec (1451.08 MB/sec)

Summary of all tests:
Operation   Max(MiB)   Min(MiB)  Mean(MiB)     StdDev    Mean(s) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt blksiz xsize aggsize API RefNum
write        2612.58    2612.58    2612.58       0.00  100.33910 0 16 1 1 1 0 1 0 0 1 17179869184 4194304 274877906944 MPIIO 0
read         1383.85    1383.85    1383.85       0.00  189.43039 0 16 1 1 1 0 1 0 0 1 17179869184 4194304 274877906944 MPIIO 0

Finished: Fri Aug  7 21:23:54 2015
IOR-3.0.1: MPI Coordinated Test of Parallel I/O

Began: Fri Aug  7 21:23:57 2015
Command line used: /cosma5/data/dr002/dc-kash1/ior/bin/ior -vvv -b 32g -f /cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS/con.ior
Machine: Linux m5140
Start time skew across all tasks: 0.00 sec

Test 0 started: Fri Aug  7 21:23:57 2015
Path: /cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS
FS: 2435.9 TiB   Used FS: 81.8%   Inodes: 1280.0 Mi   Used Inodes: 5.9%
Participating tasks: 8
Summary:
	api                = MPIIO (version=2, subversion=2)
	test filename      = /cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS/iorTest
	access             = file-per-process, independent
	pattern            = segmented (1 segment)
	ordering in a file = sequential offsets
	ordering inter file= no tasks offsets
	clients            = 8 (1 per node)
	repetitions        = 1
	xfersize           = 4 MiB
	blocksize          = 32 GiB
	aggregate filesize = 256 GiB

access    bw(MiB/s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter
------    ---------  ---------- ---------  --------   --------   --------   --------   ----
delaying 10 seconds . . .

hints passed to MPI_File_open() {
}

hints returned from opened file {
	cb_buffer_size = 4194304
	romio_cb_read = automatic
	romio_cb_write = automatic
	cb_nodes = 1
	romio_no_indep_rw = false
	ind_rd_buffer_size = 4194304
	ind_wr_buffer_size = 524288
	romio_ds_read = automatic
	romio_ds_write = automatic
	cb_config_list = *:1
}
write     3200.78    33554432   4096       0.011520   81.89      5.75       81.90      0   
delaying 10 seconds . . .

hints passed to MPI_File_open() {
}

hints returned from opened file {
	cb_buffer_size = 4194304
	romio_cb_read = automatic
	romio_cb_write = automatic
	cb_nodes = 1
	romio_no_indep_rw = false
	ind_rd_buffer_size = 4194304
	ind_wr_buffer_size = 524288
	romio_ds_read = automatic
	romio_ds_write = automatic
	cb_config_list = *:1
}
read      1458.21    33554432   4096       0.000457   179.77     11.74      179.77     0   
remove    -          -          -          -          -          -          0.016180   0   

Max Write: 3200.78 MiB/sec (3356.26 MB/sec)
Max Read:  1458.21 MiB/sec (1529.04 MB/sec)

Summary of all tests:
Operation   Max(MiB)   Min(MiB)  Mean(MiB)     StdDev    Mean(s) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt blksiz xsize aggsize API RefNum
write        3200.78    3200.78    3200.78       0.00   81.90009 0 8 1 1 1 0 1 0 0 1 34359738368 4194304 274877906944 MPIIO 0
read         1458.21    1458.21    1458.21       0.00  179.77128 0 8 1 1 1 0 1 0 0 1 34359738368 4194304 274877906944 MPIIO 0

Finished: Fri Aug  7 21:28:39 2015
IOR-3.0.1: MPI Coordinated Test of Parallel I/O

Began: Fri Aug  7 21:28:42 2015
Command line used: /cosma5/data/dr002/dc-kash1/ior/bin/ior -vvv -b 64g -f /cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS/con.ior
Machine: Linux m5140
Start time skew across all tasks: 0.00 sec

Test 0 started: Fri Aug  7 21:28:42 2015
Path: /cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS
FS: 2435.9 TiB   Used FS: 81.7%   Inodes: 1280.0 Mi   Used Inodes: 5.9%
Participating tasks: 4
Summary:
	api                = MPIIO (version=2, subversion=2)
	test filename      = /cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS/iorTest
	access             = file-per-process, independent
	pattern            = segmented (1 segment)
	ordering in a file = sequential offsets
	ordering inter file= no tasks offsets
	clients            = 4 (1 per node)
	repetitions        = 1
	xfersize           = 4 MiB
	blocksize          = 64 GiB
	aggregate filesize = 256 GiB

access    bw(MiB/s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter
------    ---------  ---------- ---------  --------   --------   --------   --------   ----
delaying 10 seconds . . .

hints passed to MPI_File_open() {
}

hints returned from opened file {
	cb_buffer_size = 4194304
	romio_cb_read = automatic
	romio_cb_write = automatic
	cb_nodes = 1
	romio_no_indep_rw = false
	ind_rd_buffer_size = 4194304
	ind_wr_buffer_size = 524288
	romio_ds_read = automatic
	romio_ds_write = automatic
	cb_config_list = *:1
}
write     3321.90    67108864   4096       0.009572   78.91      2.76       78.91      0   
delaying 10 seconds . . .

hints passed to MPI_File_open() {
}

hints returned from opened file {
	cb_buffer_size = 4194304
	romio_cb_read = automatic
	romio_cb_write = automatic
	cb_nodes = 1
	romio_no_indep_rw = false
	ind_rd_buffer_size = 4194304
	ind_wr_buffer_size = 524288
	romio_ds_read = automatic
	romio_ds_write = automatic
	cb_config_list = *:1
}
read      1724.95    67108864   4096       0.000700   151.97     1.50       151.97     0   
remove    -          -          -          -          -          -          0.011161   0   

Max Write: 3321.90 MiB/sec (3483.26 MB/sec)
Max Read:  1724.95 MiB/sec (1808.75 MB/sec)

Summary of all tests:
Operation   Max(MiB)   Min(MiB)  Mean(MiB)     StdDev    Mean(s) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt blksiz xsize aggsize API RefNum
write        3321.90    3321.90    3321.90       0.00   78.91388 0 4 1 1 1 0 1 0 0 1 68719476736 4194304 274877906944 MPIIO 0
read         1724.95    1724.95    1724.95       0.00  151.97152 0 4 1 1 1 0 1 0 0 1 68719476736 4194304 274877906944 MPIIO 0

Finished: Fri Aug  7 21:32:53 2015
IOR-3.0.1: MPI Coordinated Test of Parallel I/O

Began: Fri Aug  7 21:32:54 2015
Command line used: /cosma5/data/dr002/dc-kash1/ior/bin/ior -vvv -b 128g -f /cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS/con.ior
Machine: Linux m5140
Start time skew across all tasks: 0.00 sec

Test 0 started: Fri Aug  7 21:32:54 2015
Path: /cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS
FS: 2435.9 TiB   Used FS: 81.7%   Inodes: 1280.0 Mi   Used Inodes: 5.9%
Participating tasks: 2
Summary:
	api                = MPIIO (version=2, subversion=2)
	test filename      = /cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS/iorTest
	access             = file-per-process, independent
	pattern            = segmented (1 segment)
	ordering in a file = sequential offsets
	ordering inter file= no tasks offsets
	clients            = 2 (1 per node)
	repetitions        = 1
	xfersize           = 4 MiB
	blocksize          = 128 GiB
	aggregate filesize = 256 GiB

access    bw(MiB/s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter
------    ---------  ---------- ---------  --------   --------   --------   --------   ----
delaying 10 seconds . . .

hints passed to MPI_File_open() {
}

hints returned from opened file {
	cb_buffer_size = 4194304
	romio_cb_read = automatic
	romio_cb_write = automatic
	cb_nodes = 1
	romio_no_indep_rw = false
	ind_rd_buffer_size = 4194304
	ind_wr_buffer_size = 524288
	romio_ds_read = automatic
	romio_ds_write = automatic
	cb_config_list = *:1
}
write     2777.93    134217728  4096       0.032315   94.36      3.30       94.37      0   
delaying 10 seconds . . .

hints passed to MPI_File_open() {
}

hints returned from opened file {
	cb_buffer_size = 4194304
	romio_cb_read = automatic
	romio_cb_write = automatic
	cb_nodes = 1
	romio_no_indep_rw = false
	ind_rd_buffer_size = 4194304
	ind_wr_buffer_size = 524288
	romio_ds_read = automatic
	romio_ds_write = automatic
	cb_config_list = *:1
}
read      1318.11    134217728  4096       0.000405   198.88     5.38       198.88     0   
remove    -          -          -          -          -          -          0.444221   0   

Max Write: 2777.93 MiB/sec (2912.87 MB/sec)
Max Read:  1318.11 MiB/sec (1382.14 MB/sec)

Summary of all tests:
Operation   Max(MiB)   Min(MiB)  Mean(MiB)     StdDev    Mean(s) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt blksiz xsize aggsize API RefNum
write        2777.93    2777.93    2777.93       0.00   94.36679 0 2 1 1 1 0 1 0 0 1 137438953472 4194304 274877906944 MPIIO 0
read         1318.11    1318.11    1318.11       0.00  198.87831 0 2 1 1 1 0 1 0 0 1 137438953472 4194304 274877906944 MPIIO 0

Finished: Fri Aug  7 21:38:08 2015
/cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS 

------------------------------------------------------------
Sender: LSF System <hpcadmin@m5140>
Subject: Job 496805: <NS_FPS_MPIIO> Done

Job <NS_FPS_MPIIO> was submitted from host <cosma-c> by user <dc-kash1> in cluster <cosma-p_cluster1>.
Job was executed on host(s) <1*m5140>, in queue <cosma5>, as user <dc-kash1> in cluster <cosma-p_cluster1>.
                            <1*m5141>
                            <1*m5145>
                            <1*m5146>
                            <1*m5147>
                            <1*m5148>
                            <1*m5149>
                            <1*m5150>
                            <1*m5151>
                            <1*m5152>
                            <1*m5169>
                            <1*m5170>
                            <1*m5171>
                            <1*m5172>
                            <1*m5173>
                            <1*m5174>
</cosma/home/dr002/dc-kash1> was used as the home directory.
</cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS> was used as the working directory.
Started at Fri Aug  7 21:18:36 2015
Results reported at Fri Aug  7 21:38:08 2015

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/tcsh

#BSUB -n 16
#BSUB -x
#BSUB -R "span[ptile=1]"
#BSUB -J NS_FPS_MPIIO
#BSUB -oo NS_FPS_MPIIO.out.%J
#BSUB -eo NS_FPS_MPIIO.err.%J
#BSUB -q cosma5
#BSUB -P dr002
#BSUB -W 08:00
 
  
set TARGET="/cosma5/data/dr002/dc-kash1/IOR_DiRAC_MPIIO/NS.FPS"
set IOR="/cosma5/data/dr002/dc-kash1/ior/bin/ior"
set IOR_SCRIPT="$TARGET/con.ior"


date '+%Y%m%d%H%M%S'

pushd $TARGET


setenv  LD_LIBRARY_PATH /cosma5/data/dr002/dc-kash1/hdf5-1.8.15-patch1/lib:$LD_LIBRARY_PATH
setenv  LD_RUN_PATH /cosma5/data/dr002/dc-kash1/hdf5-1.8.15-patch1/lib:$LD_RUN_PATH
setenv  LDFLAGS "-L/cosma5/data/dr002/dc-kash1/hdf5-1.8.15-patch1/lib"


setenv HDF5_DISABLE_VERSION_CHECK 1

set numNodes= ( 16 8 4 2 1 )
set blockSize=( 16 32 64 128 256 )

foreach test (`seq 5`)
    @ test=$test - 1
    set cmd="$IOR -vvv  -b ${blockSize[$test]}g -f $IOR_SCRIPT"
    mpirun -np ${numNodes[$test]}  $IOR -vvv  -b ${blockSize[$test]}g -f $IOR_SCRIPT
end
popd



------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time   :   1598.24 sec.
    Max Memory :       225 MB
    Max Swap   :      1395 MB

    Max Processes  :        30
    Max Threads    :        30

The output (if any) is above this job summary.



PS:

Read file <NS_FPS_MPIIO.err.496805> for stderr output of this job.

